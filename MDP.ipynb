{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0348f22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action 1\n",
      "[[0. 0. 0. 0. 0. 0.]]\n",
      "bf_Q [[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "bf_max_state [0 1 2 3 4 5]\n",
      "max_state [1]\n",
      "af_max_state [1]\n",
      "max_value [[0.]]\n",
      "reward [[0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "curr_state 0\n",
      "poss_act [1 4]\n",
      "action 4\n",
      "bf_Q [[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "bf_max_state [0 1 2 3 4 5]\n",
      "max_state [4]\n",
      "af_max_state [4]\n",
      "max_value [[0.]]\n",
      "reward [[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "curr_state 3\n",
      "poss_act [1 2 4]\n",
      "action 4\n",
      "bf_Q [[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "bf_max_state [0 1 2 3 4 5]\n",
      "max_state [5]\n",
      "af_max_state [5]\n",
      "max_value [[0.]]\n",
      "reward [[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "curr_state 0\n",
      "poss_act [1 4]\n",
      "action 1\n",
      "bf_Q [[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "bf_max_state [0 1 2 3 4 5]\n",
      "max_state [0]\n",
      "af_max_state [0]\n",
      "max_value [[0.]]\n",
      "reward [[0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "curr_state 3\n",
      "poss_act [1 2 4]\n",
      "action 2\n",
      "bf_Q [[0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "bf_max_state [0 1 2 3 4 5]\n",
      "max_state [2]\n",
      "af_max_state [2]\n",
      "max_value [[0.]]\n",
      "reward [[0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "curr_state 2\n",
      "poss_act [2 3]\n",
      "action 3\n",
      "bf_Q [[0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "bf_max_state [1 2 4]\n",
      "max_state [4]\n",
      "af_max_state [4]\n",
      "max_value [[1.]]\n",
      "reward [[0.  1.  0.  0.  1.  0. ]\n",
      " [0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  1.8 0.  0. ]\n",
      " [0.  1.  1.  0.  1.  0. ]\n",
      " [0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0. ]]\n",
      "\n",
      "curr_state 1\n",
      "poss_act [1 3 5]\n",
      "action 1\n",
      "bf_Q [[0.  1.  0.  0.  1.  0. ]\n",
      " [0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  1.8 0.  0. ]\n",
      " [0.  1.  1.  0.  1.  0. ]\n",
      " [0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0. ]]\n",
      "bf_max_state [0 1 2 3 4 5]\n",
      "max_state [3]\n",
      "af_max_state [3]\n",
      "max_value [[0.]]\n",
      "reward [[0.  1.  0.  0.  1.  0. ]\n",
      " [0.  1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  1.8 0.  0. ]\n",
      " [0.  1.  1.  0.  1.  0. ]\n",
      " [0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0. ]]\n",
      "\n",
      "curr_state 4\n",
      "poss_act [0 3]\n",
      "action 3\n",
      "bf_Q [[0.  1.  0.  0.  1.  0. ]\n",
      " [0.  1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  1.8 0.  0. ]\n",
      " [0.  1.  1.  0.  1.  0. ]\n",
      " [0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0. ]]\n",
      "bf_max_state [1 2 4]\n",
      "max_state [1]\n",
      "af_max_state [1]\n",
      "max_value [[1.]]\n",
      "reward [[0.  1.  0.  0.  1.  0. ]\n",
      " [0.  1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  1.8 0.  0. ]\n",
      " [0.  1.  1.  0.  1.  0. ]\n",
      " [0.  0.  0.  1.8 0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0. ]]\n",
      "\n",
      "curr_state 3\n",
      "poss_act [1 2 4]\n",
      "action 1\n",
      "bf_Q [[0.  1.  0.  0.  1.  0. ]\n",
      " [0.  1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  1.8 0.  0. ]\n",
      " [0.  1.  1.  0.  1.  0. ]\n",
      " [0.  0.  0.  1.8 0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0. ]]\n",
      "bf_max_state [1]\n",
      "af_max_state 1\n",
      "max_value 1.0\n",
      "reward [[0.  1.  0.  0.  1.  0. ]\n",
      " [0.  1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  1.8 0.  0. ]\n",
      " [0.  1.8 1.  0.  1.  0. ]\n",
      " [0.  0.  0.  1.8 0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0. ]]\n",
      "\n",
      "curr_state 5\n",
      "poss_act [1 4]\n",
      "action 4\n",
      "bf_Q [[0.  1.  0.  0.  1.  0. ]\n",
      " [0.  1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  1.8 0.  0. ]\n",
      " [0.  1.8 1.  0.  1.  0. ]\n",
      " [0.  0.  0.  1.8 0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0. ]]\n",
      "bf_max_state [3]\n",
      "af_max_state 3\n",
      "max_value 1.8\n",
      "reward [[  0.     1.     0.     0.     1.     0.  ]\n",
      " [  0.     1.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     1.8    0.     0.  ]\n",
      " [  0.     1.8    1.     0.     1.     0.  ]\n",
      " [  0.     0.     0.     1.8    0.     0.  ]\n",
      " [  0.     0.     0.     0.   101.44   0.  ]]\n",
      "\n",
      "curr_state 2\n",
      "poss_act [2 3]\n",
      "action 2\n",
      "bf_Q [[  0.     1.     0.     0.     1.     0.  ]\n",
      " [  0.     1.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     1.8    0.     0.  ]\n",
      " [  0.     1.8    1.     0.     1.     0.  ]\n",
      " [  0.     0.     0.     1.8    0.     0.  ]\n",
      " [  0.     0.     0.     0.   101.44   0.  ]]\n",
      "bf_max_state [3]\n",
      "af_max_state 3\n",
      "max_value 1.8\n",
      "reward [[  0.     1.     0.     0.     1.     0.  ]\n",
      " [  0.     1.     0.     0.     0.     0.  ]\n",
      " [  0.     0.   101.44   1.8    0.     0.  ]\n",
      " [  0.     1.8    1.     0.     1.     0.  ]\n",
      " [  0.     0.     0.     1.8    0.     0.  ]\n",
      " [  0.     0.     0.     0.   101.44   0.  ]]\n",
      "\n",
      "curr_state 0\n",
      "poss_act [1 4]\n",
      "action 1\n",
      "bf_Q [[  0.     1.     0.     0.     1.     0.  ]\n",
      " [  0.     1.     0.     0.     0.     0.  ]\n",
      " [  0.     0.   101.44   1.8    0.     0.  ]\n",
      " [  0.     1.8    1.     0.     1.     0.  ]\n",
      " [  0.     0.     0.     1.8    0.     0.  ]\n",
      " [  0.     0.     0.     0.   101.44   0.  ]]\n",
      "bf_max_state [1]\n",
      "af_max_state 1\n",
      "max_value 1.0\n",
      "reward [[  0.     1.8    0.     0.     1.     0.  ]\n",
      " [  0.     1.     0.     0.     0.     0.  ]\n",
      " [  0.     0.   101.44   1.8    0.     0.  ]\n",
      " [  0.     1.8    1.     0.     1.     0.  ]\n",
      " [  0.     0.     0.     1.8    0.     0.  ]\n",
      " [  0.     0.     0.     0.   101.44   0.  ]]\n",
      "\n",
      "curr_state 1\n",
      "poss_act [1 3 5]\n",
      "action 5\n",
      "bf_Q [[  0.     1.8    0.     0.     1.     0.  ]\n",
      " [  0.     1.     0.     0.     0.     0.  ]\n",
      " [  0.     0.   101.44   1.8    0.     0.  ]\n",
      " [  0.     1.8    1.     0.     1.     0.  ]\n",
      " [  0.     0.     0.     1.8    0.     0.  ]\n",
      " [  0.     0.     0.     0.   101.44   0.  ]]\n",
      "bf_max_state [4]\n",
      "af_max_state 4\n",
      "max_value 101.44\n",
      "reward [[  0.      1.8     0.      0.      1.      0.   ]\n",
      " [  0.      1.      0.      0.      0.     82.152]\n",
      " [  0.      0.    101.44    1.8     0.      0.   ]\n",
      " [  0.      1.8     1.      0.      1.      0.   ]\n",
      " [  0.      0.      0.      1.8     0.      0.   ]\n",
      " [  0.      0.      0.      0.    101.44    0.   ]]\n",
      "\n",
      "curr_state 3\n",
      "poss_act [1 2 4]\n",
      "action 1\n",
      "bf_Q [[  0.      1.8     0.      0.      1.      0.   ]\n",
      " [  0.      1.      0.      0.      0.     82.152]\n",
      " [  0.      0.    101.44    1.8     0.      0.   ]\n",
      " [  0.      1.8     1.      0.      1.      0.   ]\n",
      " [  0.      0.      0.      1.8     0.      0.   ]\n",
      " [  0.      0.      0.      0.    101.44    0.   ]]\n",
      "bf_max_state [5]\n",
      "af_max_state 5\n",
      "max_value 82.152\n",
      "reward [[  0.       1.8      0.       0.       1.       0.    ]\n",
      " [  0.       1.       0.       0.       0.      82.152 ]\n",
      " [  0.       0.     101.44     1.8      0.       0.    ]\n",
      " [  0.      66.7216   1.       0.       1.       0.    ]\n",
      " [  0.       0.       0.       1.8      0.       0.    ]\n",
      " [  0.       0.       0.       0.     101.44     0.    ]]\n",
      "\n",
      "curr_state 5\n",
      "poss_act [1 4]\n",
      "action 4\n",
      "bf_Q [[  0.       1.8      0.       0.       1.       0.    ]\n",
      " [  0.       1.       0.       0.       0.      82.152 ]\n",
      " [  0.       0.     101.44     1.8      0.       0.    ]\n",
      " [  0.      66.7216   1.       0.       1.       0.    ]\n",
      " [  0.       0.       0.       1.8      0.       0.    ]\n",
      " [  0.       0.       0.       0.     101.44     0.    ]]\n",
      "bf_max_state [3]\n",
      "af_max_state 3\n",
      "max_value 1.8\n",
      "reward [[  0.       1.8      0.       0.       1.       0.    ]\n",
      " [  0.       1.       0.       0.       0.      82.152 ]\n",
      " [  0.       0.     101.44     1.8      0.       0.    ]\n",
      " [  0.      66.7216   1.       0.       1.       0.    ]\n",
      " [  0.       0.       0.       1.8      0.       0.    ]\n",
      " [  0.       0.       0.       0.     101.44     0.    ]]\n",
      "\n",
      "curr_state 4\n",
      "poss_act [0 3]\n",
      "action 3\n",
      "bf_Q [[  0.       1.8      0.       0.       1.       0.    ]\n",
      " [  0.       1.       0.       0.       0.      82.152 ]\n",
      " [  0.       0.     101.44     1.8      0.       0.    ]\n",
      " [  0.      66.7216   1.       0.       1.       0.    ]\n",
      " [  0.       0.       0.       1.8      0.       0.    ]\n",
      " [  0.       0.       0.       0.     101.44     0.    ]]\n",
      "bf_max_state [1]\n",
      "af_max_state 1\n",
      "max_value 66.72160000000001\n",
      "reward [[  0.        1.8       0.        0.        1.        0.     ]\n",
      " [  0.        1.        0.        0.        0.       82.152  ]\n",
      " [  0.        0.      101.44      1.8       0.        0.     ]\n",
      " [  0.       66.7216    1.        0.        1.        0.     ]\n",
      " [  0.        0.        0.       54.37728   0.        0.     ]\n",
      " [  0.        0.        0.        0.      101.44      0.     ]]\n",
      "\n",
      "curr_state 0\n",
      "poss_act [1 4]\n",
      "action 1\n",
      "bf_Q [[  0.        1.8       0.        0.        1.        0.     ]\n",
      " [  0.        1.        0.        0.        0.       82.152  ]\n",
      " [  0.        0.      101.44      1.8       0.        0.     ]\n",
      " [  0.       66.7216    1.        0.        1.        0.     ]\n",
      " [  0.        0.        0.       54.37728   0.        0.     ]\n",
      " [  0.        0.        0.        0.      101.44      0.     ]]\n",
      "bf_max_state [5]\n",
      "af_max_state 5\n",
      "max_value 82.152\n",
      "reward [[  0.       66.7216    0.        0.        1.        0.     ]\n",
      " [  0.        1.        0.        0.        0.       82.152  ]\n",
      " [  0.        0.      101.44      1.8       0.        0.     ]\n",
      " [  0.       66.7216    1.        0.        1.        0.     ]\n",
      " [  0.        0.        0.       54.37728   0.        0.     ]\n",
      " [  0.        0.        0.        0.      101.44      0.     ]]\n",
      "\n",
      "curr_state 5\n",
      "poss_act [1 4]\n",
      "action 1\n",
      "bf_Q [[  0.       66.7216    0.        0.        1.        0.     ]\n",
      " [  0.        1.        0.        0.        0.       82.152  ]\n",
      " [  0.        0.      101.44      1.8       0.        0.     ]\n",
      " [  0.       66.7216    1.        0.        1.        0.     ]\n",
      " [  0.        0.        0.       54.37728   0.        0.     ]\n",
      " [  0.        0.        0.        0.      101.44      0.     ]]\n",
      "bf_max_state [5]\n",
      "af_max_state 5\n",
      "max_value 82.152\n",
      "reward [[  0.       66.7216    0.        0.        1.        0.     ]\n",
      " [  0.        1.        0.        0.        0.       82.152  ]\n",
      " [  0.        0.      101.44      1.8       0.        0.     ]\n",
      " [  0.       66.7216    1.        0.        1.        0.     ]\n",
      " [  0.        0.        0.       54.37728   0.        0.     ]\n",
      " [  0.       66.7216    0.        0.      101.44      0.     ]]\n",
      "\n",
      "curr_state 3\n",
      "poss_act [1 2 4]\n",
      "action 4\n",
      "bf_Q [[  0.       66.7216    0.        0.        1.        0.     ]\n",
      " [  0.        1.        0.        0.        0.       82.152  ]\n",
      " [  0.        0.      101.44      1.8       0.        0.     ]\n",
      " [  0.       66.7216    1.        0.        1.        0.     ]\n",
      " [  0.        0.        0.       54.37728   0.        0.     ]\n",
      " [  0.       66.7216    0.        0.      101.44      0.     ]]\n",
      "bf_max_state [3]\n",
      "af_max_state 3\n",
      "max_value 54.37728000000001\n",
      "reward [[  0.        66.7216     0.         0.         1.         0.      ]\n",
      " [  0.         1.         0.         0.         0.        82.152   ]\n",
      " [  0.         0.       101.44       1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       101.44       0.      ]]\n",
      "\n",
      "curr_state 1\n",
      "poss_act [1 3 5]\n",
      "action 3\n",
      "bf_Q [[  0.        66.7216     0.         0.         1.         0.      ]\n",
      " [  0.         1.         0.         0.         0.        82.152   ]\n",
      " [  0.         0.       101.44       1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       101.44       0.      ]]\n",
      "bf_max_state [1]\n",
      "af_max_state 1\n",
      "max_value 66.72160000000001\n",
      "reward [[  0.        66.7216     0.         0.         1.         0.      ]\n",
      " [  0.         1.         0.        54.37728    0.        82.152   ]\n",
      " [  0.         0.       101.44       1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       101.44       0.      ]]\n",
      "\n",
      "curr_state 0\n",
      "poss_act [1 4]\n",
      "action 1\n",
      "bf_Q [[  0.        66.7216     0.         0.         1.         0.      ]\n",
      " [  0.         1.         0.        54.37728    0.        82.152   ]\n",
      " [  0.         0.       101.44       1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       101.44       0.      ]]\n",
      "bf_max_state [5]\n",
      "af_max_state 5\n",
      "max_value 82.152\n",
      "reward [[  0.        66.7216     0.         0.         1.         0.      ]\n",
      " [  0.         1.         0.        54.37728    0.        82.152   ]\n",
      " [  0.         0.       101.44       1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       101.44       0.      ]]\n",
      "\n",
      "curr_state 2\n",
      "poss_act [2 3]\n",
      "action 2\n",
      "bf_Q [[  0.        66.7216     0.         0.         1.         0.      ]\n",
      " [  0.         1.         0.        54.37728    0.        82.152   ]\n",
      " [  0.         0.       101.44       1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       101.44       0.      ]]\n",
      "bf_max_state [2]\n",
      "af_max_state 2\n",
      "max_value 101.44\n",
      "reward [[  0.        66.7216     0.         0.         1.         0.      ]\n",
      " [  0.         1.         0.        54.37728    0.        82.152   ]\n",
      " [  0.         0.       181.152      1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       101.44       0.      ]]\n",
      "\n",
      "curr_state 1\n",
      "poss_act [1 3 5]\n",
      "action 5\n",
      "bf_Q [[  0.        66.7216     0.         0.         1.         0.      ]\n",
      " [  0.         1.         0.        54.37728    0.        82.152   ]\n",
      " [  0.         0.       181.152      1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       101.44       0.      ]]\n",
      "bf_max_state [4]\n",
      "af_max_state 4\n",
      "max_value 101.44\n",
      "reward [[  0.        66.7216     0.         0.         1.         0.      ]\n",
      " [  0.         1.         0.        54.37728    0.        82.152   ]\n",
      " [  0.         0.       181.152      1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       101.44       0.      ]]\n",
      "\n",
      "curr_state 3\n",
      "poss_act [1 2 4]\n",
      "action 1\n",
      "bf_Q [[  0.        66.7216     0.         0.         1.         0.      ]\n",
      " [  0.         1.         0.        54.37728    0.        82.152   ]\n",
      " [  0.         0.       181.152      1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       101.44       0.      ]]\n",
      "bf_max_state [5]\n",
      "af_max_state 5\n",
      "max_value 82.152\n",
      "reward [[  0.        66.7216     0.         0.         1.         0.      ]\n",
      " [  0.         1.         0.        54.37728    0.        82.152   ]\n",
      " [  0.         0.       181.152      1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       101.44       0.      ]]\n",
      "\n",
      "curr_state 5\n",
      "poss_act [1 4]\n",
      "action 4\n",
      "bf_Q [[  0.        66.7216     0.         0.         1.         0.      ]\n",
      " [  0.         1.         0.        54.37728    0.        82.152   ]\n",
      " [  0.         0.       181.152      1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       101.44       0.      ]]\n",
      "bf_max_state [3]\n",
      "af_max_state 3\n",
      "max_value 54.37728000000001\n",
      "reward [[  0.        66.7216     0.         0.         1.         0.      ]\n",
      " [  0.         1.         0.        54.37728    0.        82.152   ]\n",
      " [  0.         0.       181.152      1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       143.501824   0.      ]]\n",
      "\n",
      "curr_state 0\n",
      "poss_act [1 4]\n",
      "action 4\n",
      "bf_Q [[  0.        66.7216     0.         0.         1.         0.      ]\n",
      " [  0.         1.         0.        54.37728    0.        82.152   ]\n",
      " [  0.         0.       181.152      1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       143.501824   0.      ]]\n",
      "bf_max_state [3]\n",
      "af_max_state 3\n",
      "max_value 54.37728000000001\n",
      "reward [[  0.        66.7216     0.         0.        44.501824   0.      ]\n",
      " [  0.         1.         0.        54.37728    0.        82.152   ]\n",
      " [  0.         0.       181.152      1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       143.501824   0.      ]]\n",
      "\n",
      "[[  0.        66.7216     0.         0.        44.501824   0.      ]\n",
      " [  0.         1.         0.        54.37728    0.        82.152   ]\n",
      " [  0.         0.       181.152      1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       143.501824   0.      ]]\n",
      "Normed Q : [[  0.        66.7216     0.         0.        44.501824   0.      ]\n",
      " [  0.         1.         0.        54.37728    0.        82.152   ]\n",
      " [  0.         0.       181.152      1.8        0.         0.      ]\n",
      " [  0.        66.7216     1.         0.        44.501824   0.      ]\n",
      " [  0.         0.         0.        54.37728    0.         0.      ]\n",
      " [  0.        66.7216     0.         0.       143.501824   0.      ]]\n",
      "[[  0.          36.83183183   0.           0.          24.56601307\n",
      "    0.        ]\n",
      " [  0.           0.55202261   0.          30.01748808   0.\n",
      "   45.34976153]\n",
      " [  0.           0.         100.           0.9936407    0.\n",
      "    0.        ]\n",
      " [  0.          36.83183183   0.55202261   0.          24.56601307\n",
      "    0.        ]\n",
      " [  0.           0.           0.          30.01748808   0.\n",
      "    0.        ]\n",
      " [  0.          36.83183183   0.           0.          79.21625155\n",
      "    0.        ]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[False False False  True False  True]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"In these code you will able to understand how the MDP works, Beofre that for your informaton you can take a look at\n",
    "https://www.csie.ntu.edu.tw/~b91046/meeting/m2/mdp.pdf for your concern about the Bellman equation \n",
    "\n",
    "What is q-learning?\n",
    "Q-learning is an off policy reinforcement learning algorithm that seeks to find the best action to take given the current state\n",
    "\n",
    "It’s considered off-policy because the q-learning function learns from actions that are outside the current policy,\n",
    "like taking random actions, and therefore a policy isn’t needed. More specifically, q-learning seeks to learn a policy that \n",
    "maximizes the total reward.\n",
    "\n",
    "The ‘q’ in q-learning stands for quality. Quality in this case represents how useful a given action is in gaining some future reward.\n",
    "\n",
    "So then here we will see q learing  reinforcement learning with reward function\n",
    "\n",
    "in these code we will import numpy library as ql\n",
    "\n",
    "ql is an nxp matrix from A for QL decomposition\n",
    "\n",
    "A is list of two matrices Q and L so that A = QL\n",
    "\n",
    "Q - nxp matrix with orthonormal columns with the same span as A\n",
    "\n",
    "L - is a lower triangular pxp matrix with nonnegative diagonal entries\n",
    "________________________________________________________________________________________________________________________________________________________________________________\n",
    "For MDP we will use two variable R as reward and Q as learininf Matrix\n",
    "\n",
    "R - is reward matrix for each state\n",
    "\n",
    "Q - is the Learning Matrix in which rewards will be learned/stored\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "import numpy as ql\n",
    "\n",
    "# you can give your own matrix\n",
    "\n",
    "R = ql.matrix([ [0,1,0,0,1,0],\n",
    "                [0,1,0,1,0,1],\n",
    "                [0,0,100,1,0,0],\n",
    "                [0,1,1,0,1,0],\n",
    "                [1,0,0,1,0,0],\n",
    "                [0,1,0,0,100,0] ])\n",
    "\n",
    "\n",
    "Q = ql.matrix(ql.zeros([6,6]))\n",
    "\n",
    "\"\"\"\n",
    "Gamma: gamma or γ is a discount factor. It’s used to balance immediate and future reward. From our update rule \n",
    "above you can see that we apply the discount to the future reward. Typically this value can range anywhere from 0.8 to 0.99.\n",
    "\n",
    "\"\"\"\n",
    "gamma = 0.8\n",
    "\n",
    "\"\"\"\n",
    "agent_s_state is the agent the name of the system calculating and s is the state and can be random number or it can be chosen\n",
    "and random is part of this stochastic process\n",
    "\"\"\"\n",
    "agent_state = 3\n",
    "\n",
    "# The possible \"a\" actions when the agent is in a given state\n",
    "def possible_actions(state):\n",
    "    current_state_row = R[state,]\n",
    "    possible_act = ql.asarray(current_state_row > 0).nonzero()\n",
    "    return possible_act[1] # Return the index of posibility\n",
    "\n",
    "# Get action\n",
    "PossibleAction = possible_actions(agent_s_state)\n",
    "\n",
    "# This function chooses at random which action to be performed within the range \n",
    "# of all the available actions.\n",
    "def ActionChoice(available_actions_range):\n",
    "    if(sum(PossibleAction) > 0):\n",
    "        next_action = int(ql.random.choice(PossibleAction,1))\n",
    "    if(sum(PossibleAction) <= 0):\n",
    "        next_action = int(ql.random.choice(5,1))\n",
    "    return next_action\n",
    "\n",
    "\n",
    "# Sample next action to be performed\n",
    "action = ActionChoice(PossibleAction)\n",
    "print('action',action)\n",
    "print(Q[action,])\n",
    "\n",
    "\"\"\"\n",
    "A version of Bellman's equation for reinforcement learning using the Q function\n",
    "This reinforcement algorithm is a memoryless process. The transition function T from one state to another\n",
    "is not in the equation below. T is done by the random choice above \n",
    "print(Q[action,])\n",
    "\n",
    "\"\"\"\n",
    "# \n",
    "def reward(current_state, action, gamma):\n",
    "    print('bf_Q',Q)\n",
    "\n",
    "    Max_State = ql.asarray(Q[action,] == ql.max(Q[action,])).nonzero()[1]\n",
    "    print('bf_max_state',Max_State)\n",
    "    if Max_State.shape[0] > 1: # Check if max_state have more than 1 column\n",
    "        Max_State = ql.random.choice(Max_State, size=1)\n",
    "        print('max_state', Max_State)\n",
    "    else:\n",
    "        Max_State = int(Max_State)\n",
    "    print('af_max_state',Max_State)\n",
    "    MaxValue = Q[action, Max_State]\n",
    "    print('max_value', MaxValue)\n",
    "    # Bellman's MDP based Q function\n",
    "    Q[current_state, action] = R[current_state, action] + gamma * MaxValue\n",
    "\n",
    "# Rewarding the Q matrix\n",
    "reward(agent_s_state, action, gamma)\n",
    "print('reward',Q[agent_s_state,])\n",
    "print('')\n",
    "# Learning over n iterations depending on the convergence of the system\n",
    "# A convergence function can replace the systematic repeating of the process\n",
    "# by comparing the sum of the Q matrix to that of Q matrix n-1 in the\n",
    "# previous episode\n",
    "for i in range(25):\n",
    "    current_state = ql.random.randint(0, int(Q.shape[0]))\n",
    "    print('curr_state',current_state)\n",
    "    PossibleAction = possible_actions(current_state)\n",
    "    print('poss_act',PossibleAction)\n",
    "    action = ActionChoice(PossibleAction)\n",
    "    print('action',action)\n",
    "    reward(current_state,action,gamma)\n",
    "    print('reward',Q)\n",
    "    print('')\n",
    "\n",
    "#Displaying Q before the norm of Q phase\n",
    "print(Q)\n",
    "\n",
    "#Norm of Q\n",
    "print(\"Normed Q :\", Q)\n",
    "print(Q/ql.max(Q)*100)\n",
    "\n",
    "#adexe\n",
    "\n",
    "state = ql.array([[0, 0, 0, 1, 0, 1]])\n",
    "print(type(state))\n",
    "print(ql.asarray(state > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3140933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960ca9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fbacd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
